\section{Experiment}
%\begin{figure}[t]
%	\centering
%	%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%	\includegraphics[width=0.8\linewidth]{experiment.png}
%	
%	\caption{Assessing the Similarity Between Network Activation and Brain Activation.}
%	\label{fig:fig3}
%\end{figure}
%\label{sec:experiment}

\subsection{Datasets} \label{sec:Dataset}

\hspace{1pc}We conduct training and testing on Carla~\cite{Dosovitskiy:2017},
and assess the proposed BID model using existing standard datasets\cite{codevilla2019exploring}. 
Each dataset specifies its own training scenes and weather conditions for data collection, 
and tests the BID's performance in scenes never seen before. 
In behavior cloning dataset\cite{codevilla2019exploring}, they focuses on transfer ability from Town01, a small scene with numerous T junctions and a variety of buildings, to Town02, a variant of Town01 with a mixture of residential and commercial buildings.
The offical test suit presents a challenging transfer problem across 6 scenes with various scenarios, including stop signs, lane changes, US-style junctions and roundabouts.
Based on the behavior cloning dataset~\cite{codevilla2019exploring}, we test the transfer ability between different weather conditions, though only two training weather types are evaluated to save computational resources. 
The behavior cloning benchmark includes three actor amount levels for each scene from less to more. 
A crowded configuration in behavior cloning is utilized, which uses a suitable number of actors to prevent typical jam with a large number of actors. 
In the offical test suit, we adjust actor number to meet the requirement of configuration. 


% CILv2
As with the newest state-of-art model~\cite{Hu:2022}, we use the driving expert driver referred to reinforcement learning imitation~\cite{Zhang:2021} using online dataset with simulator.
This end-to-end method with reinforced feature exhibits very real action compared to a well-designed model in traffic scene. 
It is important to notice that the huminine driving demonstration will be used as training data in real world tests.
The standard configuration in~\cite{Zhang:2021} is used, which means that it is similar to the student driver in RL-Coach\cite{Zhang:2021} and MBI~\cite{Hu:2022}, the driving agent is the model found in simulator.
The frame from all the 3 horizontally concatenated vehicle-based RGB image sensors is 900 wide and 300 high, with a field of view of $180^{\circ}$.
The cameras are arranged without overlap, collectively covering a total filed of field.


Using the experienced agent, vehicle-based cameras and hero vehicle, driving data is collected in progressively more crowed scenarios.
Firstly, data is collected in Carla's Town01, a small scene that allows contains one lane merely, meaning we can't change lanes.
% 中午、傍晚、大雨的中午、潮湿的中午
Specifically, 1620K frames approximately at 10 fps from vehicle-based RGB sensor is utilized on 4 different learning weather types.
%: ClearNoon, ClearSunset, HardRainNoon, and WetNoon. 
% 测试：Town02 细雨的傍晚、潮湿的傍晚
For generalization testing, we use Carla's Town02, evaluating the agent under SoftRainSunset and WetSunset weather conditions. 
Next, the benchmark is collected on several simulator scene to capture crowed driving scenes, such as T junctions, highway entrances, exits, and crossing the intersections. 
To maintain consistency with setup in MBI~\cite{Hu:2022}, Town05 is reserved for testing, and totaling approximately 2100K image per sensor is gathered at 10 fps in Town01 to Town06. 
%In Town01 and Town02, both training and testing weather conditions remain the same.



\subsection{Implementation}

\hspace{1pc}The AdamW optimizer~\cite{loshchilov2017decoupled} with a step size of $2 \times 10^{-4}$ at each iteration and weight decay of $0.02$ is utilized to training the BID model. 
We reduce the learning rate by half at 25, 45, and 70 epochs.
The batch size is 100, 
and model training is performed for 100 epochs on a single NVIDIA RTX A6000 GPU.


% Cornet详细信息
In ventral stream, after the input is processed by V2$_\text{COR}$ once, the resulting output is inputted back to V2$_\text{COR}$.
This process forms a recurrent connection.
We run V2$_\text{COR}$ and IT$_\text{COR}$ two times, and execute V4$_\text{COR}$ four times repeatedly, respectively.
This configuration yielded a best model performance based on our evaluation scores.
Similar to ResNet-50, a convolutional operation is accompanied by layer normalization and a rectified linear unit.
Layer normalization is unique to each time step.
The existing description of the ventral pathway does not include the connection that span across different regions, and retinal and lateral geniculate nucleus processing are not explicitly modeled.


% 各种环境下的场景说明
% Roach_carla0913_fps10_dense_normalcamera_NoCrash_3cam/RouteScenario_0000/rgb_central001215.png
\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{fig/dataset_examples.pdf}
	\caption{Top: When the ego-vehicle is driving on the road, even though the triffic light is green, the vehicle in front has stopped, but the ego-vehicle still brakes.
		Bottom: When the vehicle is moving and encounters a traffic jam in front, it will automaitcally brake and stop.
		The brake command in the figure is displayed in \textcolor{red}{red}.}
	\label{fig:command_ambiguous}
\end{figure}

% 驾驶评估的3个不同数据集（不包含实验结果）
\subsection{Driving Evaluation}
\label{sec:Metrics}
\hspace{1pc}The BID agent's capabilities are constrained by the proficiency of the expert it is emulating. 
The success of the agent hinges on the expertise and skill level of its human counterpart.
If the expert performs poorly, the agent that mimics it will also show suboptimal results. 
The BID model is designed to closely follow the visual information processing system of the human brain, both structurally and functionally. 
When optimized, the model's output closely matches the expert's performance, as evaluated by similarity metrics.


We conduct experiments on small single-lane town (Sec.~\ref{sec:small_town_results}) following the nocrash dataset~\cite{Zhang:2021,Hu:2022}, and on multiple towns (Sec.~\ref{sec:small_town_results}) based on the offline Carla leaderboard dataset~\cite{Zhang:2021,Hu:2022}. 


% 三种不同场景的性能
\begin{table*}
	\caption{Results in Town02.
		The expert refers to reinforcement learning~\cite{Zhang:2021}, while RL-Coach refers to Roach imitation learning. 
		Carla is used to evaluate all methods.
		3 experiment with various random seeds are used to calculate the mean values and standard deviations.
		Better performance is indicated by higher values for metrics marked with $\uparrow$, while lower are perferred for those marked with $\downarrow$.}
	\centering
	\resizebox{0.95\linewidth}{!}{
		\begin{tabular}{@{}lccc|ccc|ccc@{}}
			\hline
			& \multicolumn{3}{c}{None} & \multicolumn{3}{c}{Normal} & \multicolumn{3}{c}{Crowded}  \\
			
			& \textbf{$\uparrow$ SR(\%)} & \textbf{$\uparrow$ S.SR(\%)} & $\downarrow$ T.L  & \textbf{$\uparrow$ SR(\%)} & \textbf{$\uparrow$ S.SR(\%)} & $\downarrow$ T.L & \textbf{$\uparrow$ SR(\%)} & \textbf{$\uparrow$ S.SR(\%)} & $\downarrow$ C.V \\
			\hline
			
			RL-Coach\cite{Zhang:2021}  & $100\pm0.0$ & $85\pm1.2$ & $66\pm5.0$ & $97\pm2.3$ & $86\pm7.2$ & $66\pm54$ & $81\pm5.0$& $68\pm7.2$ & $63\pm52.7$\\
			CIL++\cite{xiao2023scaling} & $100\pm0.0$ & $100\pm0.0$ & $0\pm0.0$  & $99\pm2.3$ & $97\pm3.1$ & $7\pm7.9$ & $83\pm7.6$ & $77\pm7.6$ & $45\pm21.5$ \\
			Our BID & $100\pm0.0$ & $100\pm0.0$ & $0\pm0.0$  & $99\pm2.3$ & $97\pm3.1$ & $7\pm7.9$ & $83\pm7.6$ & $77\pm7.6$ & $45\pm21.5$ \\
			\hline
			Expert & $100\pm0.0$ & $100\pm0.0$ & $0\pm0.0$ & $100\pm0.0$ & $97\pm0.0$ & $13\pm4.6$ & $84\pm2.0$ & $82\pm2.0$ & $37\pm14.1$ \\
			\hline 
		\end{tabular}
	}
	\label{tab:T2_NC_results}
\end{table*}


\begin{table*}
	\caption{Town05 outcomes based on offline measurements.
		Carla is used to evaluate all methods.
		3 experiment with various random seeds are used to calculate the mean values and standard deviations.
		Better performance is indicated by higher values for metrics marked with $\uparrow$, while lower are perferred for those marked with $\downarrow$.}
	\centering
	\resizebox{0.88\linewidth}{!}{
		\begin{tabular}{@{}lccccccccccccccccccccc@{}}
			\hline
			%   & SR(\%) 
			%   & S.SR(\%) 
			& \textbf{$\uparrow$ Avg.RC(\%)} & \textbf{$\uparrow$ Avg.DS }
			& $\downarrow$ C.V & $\downarrow$ C.L & $\downarrow$ T.L & $\downarrow$ O.L & $\downarrow$ R.Dev 
			\\
			\hline
			RL-Coach\cite{Zhang:2021}  & $92\pm3.1$ & $51\pm7.9$ 
			& $7.5\pm1.3$ & $4.3\pm1.6$ & $26.0\pm8.9$ & $5.4\pm2.7$ & $3.0\pm3.2$ & \\
			MBI~\cite{Hu:2022}
			& $98\pm2.2$ & $73\pm2.9$ 
			& $6.0\pm3.7$ & $0.0\pm0.0$ & $3.6\pm3.8$ & $3.5\pm1.5$ & $0.0\pm0.0$ \\   
			CIL++\cite{xiao2023scaling} 
			& $98\pm1.7$ & $68\pm2.7$ 
			& $6.0\pm0.5$ & $3.8\pm0.7$ & $5.8\pm5.1$ & $6.1\pm2.2$ & $9.4\pm3.6$ \\
			Our BID 
			& $98\pm1.7$ & $68\pm2.7$ 
			& $6.0\pm0.5$ & $3.8\pm0.7$ & $5.8\pm5.1$ & $6.1\pm2.2$ & $9.4\pm3.6$ \\
			\hline
			Expert 
			& $99\pm0.8$ & $89\pm1.7$ 
			& $3.2\pm1.1$ & $0.0\pm0.0$ & $1.3\pm0.4$ & $0.0\pm0.0$ & $0.0\pm0.0$ \\
			\hline
		\end{tabular}
	}
	\label{tab:T5_results}
\end{table*}

\subsubsection{Nocrash Benchmark Evaluation}\label{nocrash_metrics}

\hspace{1pc}Based on the quantity of variational actors ({\ie}, human, cars) in Carla scene, the benchmark comprises 3 mission with varying degrees of challenge: crowded, normal and none.
Town02 specifies that there are zero human and zeros cars (none); fifty human and fifteen cars (normal); and one hundred fifty human and seventy cars (crowded).
%
% 默认的配置会导致拥堵死锁
The initial actors quantity in nocrash benchmark frequently causes crowded and congestion at intersections in the crowded scenario~\cite{Zhang:2021}.
To address this, we adopt the \emph{crowded} scenarios described in RL-Coach~\cite{Zhang:2021}, reducing human population from one hundred and fifty to seventy. 
A task consist of two weather conditions never seen it before and twenty-five goal-oriented scenarios.
If a crash happends, the simulated case ends and considered as a fail.
The driving rate is penalized for unexpected violations in accordance with the guidelines outlined in these benchmark.


% 度量标准的说明
The success rate (\emph{SR}), which represents the proportion of runs that are normally finished, is the primary evaluation index for comparing agents.
% 严格的成功率
We also offer the rigorous SR (\emph{S.SR}) for a more detailed measurement, which calculates the proportion of normal runs under a 0 violation policy for any rules, like running at a no passing traffic sign or deviating from planned route.
% T.L：在红灯时不停的次数
Additionally, we include other violative measurement.
The quantity of the vehicle fails to stop at a red traffic sign is represented by \emph{T.L}.
% 和其他车辆碰撞的次数
The quantity of crashes with other cars is denoted by \emph{C.V}.
% R.Dev：当高层命令没有很好地执行，路线偏差的次数
The quantity of path inconformity where the high level navigation is not properly ran is denoted by \emph{R.Dev}.
% O.L：考虑了自主车辆驶出车道的情况（例如，在对面车道或人行道上）
\emph{O.L} is the quantity of the agent deviate from its lane ({\eg}, to the sidewalk or to the opposite lane).
% C.L：与城镇布局发生碰撞的次数
The quantity of crashes with scene is symbolized by \emph{C.L}.
Per kilometer driving, we normalize all the number of violations.


\subsubsection{Offline Leaderboard Metrics.}\label{lb_metrics}
\hspace{1pc}To align our evaluation with Carla leaderboard\cite{Hu:2022}, we use the offline Carla leaderboard metrics for multiple towns. 
% Avg.DS 平均驾驶分数、平均路线完成
The key metrics are the average driving score (\emph{Avg.DS}) and the average route completion (\emph{Avg.RC}). 
\emph{Avg.DS} penalizes driving performance based on the criteria defined in the Carla leaderboard, 
% Avg.RC：自车辆能够行驶至目标的平均距离。
while \emph{Avg.RC} measures the average distance the ego-vehicles is able to travel toward the goal.


% 高层导航命令
\subsubsection{High-level Navigation Commands} 
\hspace{1pc}As in CILRS \cite{Codevilla:2019}, during training, we use simple navigation commands such as ``continue in the lane" or ``go-straight/turn-left/turn-right" when reaching an intersection.
However, in more complex towns, after crossing an intersection, the ego-vehicle may legally enter any of the multiple available lanes. 
Since this information is avaiable through the global navigation system, if the ego-vehicle deviates from the pre-planned trajectory by entering a different lane, a corrective command is issued, such as ``move to left lane" or ``move to right lane" as soon as possible. 
This corrective mechanism is only applied during testing. 
An example of this process is shown in Fig.~\ref{fig:command_ambiguous}.



\subsection{Experimental Results}
\label{sec:Results}
% 基于模型的模仿学习
\hspace{1pc}We compare BID with three state-of-the-art, vision-based end-to-end autonomous driving models: the Roach IL model (here RL-Coach) \cite{Zhang:2021}, MBI~\cite{Hu:2022} and CIL++\cite{xiao2023scaling}. 
It is important to note that although BID uses data generated by the Roach RL model, training a BID model does not require human-labeled sensor data. 
In this case, the Roach RL model acts as a human driver during data acquisition. 
In contrast, training a RL-Coach model requires supervision from the Roach RL expert, who is trained with privileged information, while MBI is trained using semantic Bird's Eye View (BeV) data for supervision.


\subsubsection{Small Single-lane Towns} \label{sec:small_town_results}

\hspace{1pc}We first conduct initial experiments using Carla's Town01 and Town02, along with the nocrash evaluations(Sec.~\ref{nocrash_metrics}). 
Town01 is used for training, while Town02 is reserved for testing (Sec.~\ref{sec:Dataset}). 
MBI provides a model trained on multiple Carla towns, but no model is available that is specifically trained on Town01 alone. 
On the other hand, RL-Coach has models trained on both Town01 and multiple towns. To ensure a fair comparison, we only use RL-Coach's model trained on Town01. 
In Table~\ref{tab:T2_NC_results}, we present the success rate (SR) and strict success rate (S.SR) for the different traffic densities (none, normal, crowded). 
For a more focused evaluation, we report T.L (traffic light violations) for the none and normal cases, and C.V (collisions with vehicles) only for the crowded case in Table~\ref{fig:score_eu_lb_tt_tn}. 
It is important to note that scenarios with fewer or no dynamic obstacles are more effective for evaluating the ego-vehicle's reaction to red traffic lights, while collisions are more appropriately assessed in scenarios with higher traffic density and more dynamic objects.


In general, BID achieves the best results across all tasks. 
In the none case, BID significantly outperforms RL-Coach in avoiding traffic light infractions, contributing to a higher S.SR.
This conclusion holds true in the normal case as well. 
In the crowded case, BID comes close to matching the expert's performance, again outperforming RL-Coach in terms of S.SR and resulting in fewer collisions with vehicles. 
For the expert, failure cases in crowded scenarios are primarily due to traffic deadlocks, which cause a timeout in route completion. 
Despite this, the expert's performance can still be considered a valid upper bound.


\subsubsection{Multi-town Generalization}\label{sec:multi_towns_result}
\hspace{1pc}In this section, we evaluate the performance of BID in more complex scenarios provided by Carla's multiple towns. 
As outlined in Sec.~\ref{sec:Dataset}, for a fair comparison, we align the training and testing settings with those used in MBI~\cite{Hu:2022}, and use Carla's offline Leaderboard metrics (Sec.~\ref{lb_metrics}). 
The results for all models trained on multi-town data are presented in Table~\ref{tab:T5_results}. 
RL-Coach performs the worst among the three models, incurring more infractions and resulting in a significantly lower Avg.DS. 
BID achieves an Avg.RC of 98\%, which is comparable to MBI. 
However, in terms of Avg.DS, MBI scores the highest at 73\%, while BID achieves 68\%. 
We attribute this difference to the fact that MBI rarely drives outside the pre-planned lane, as it is provided with the route map as input. 
In contrast, BID does not use an explicit route map and instead relies solely on high-level navigation commands.

% 不同迭代时的消融实验
% CILv2_multiview\network\models\architectures\CIL_multiview\evaluator.py
\begin{figure*}[t]
	\centering
	\includegraphics[width=0.99\textwidth]{fig/driving_scores.pdf}
	\vspace{-1ex}
	\caption{\textbf{Driving performance and training progress of BID.} 
		All BID agents are evaluated in LBCRoutes after 30 peoch.
		Top figure: The ground truth and model prediction for steering angles are shown in green and blue, respectively.
		Bottom figure: The errors in training progress are displayed every 5 epochs, including the mean absolute error (MAE) for steering and acceleration.
		Results are reported as the mean over 5 evaluation seeds, while other metrics are evaluated with single seed. 
		The offline Leaderboard benchmark is used for evaluation.}
	\vspace{-1.5ex}
	\label{fig:score_eu_lb_tt_tn}
\end{figure*}


% 在密集场景中的消融实验
% table:sucess_rate_nc_dense
\begin{table}
	\caption{\textbf{Success rate of camera-based end-to-end imitation learning agents on nocrash-crowded.}
		The mean and standard deviation are computed over 5 seeds. 
		% 数据聚合方法：DAGGER：试图在学习策略诱导的状态分布下收集专家演示
		Our models are from DAGGER iteration 5. 
		For DA-RB, ``+" indicates triangular perturbations have been added to the off-policy dataset, ``(E)" denotes an ensemble of all iterations.}
	\setlength{\tabcolsep}{6.67pt}
	\centering
	\begin{tabular}{lccccc}
		\hline
		Success Rate \% $\uparrow$
		&  NCd-tt & NCd-tn  & NCd-nt & NCd-nn  \\ 
		\hline
		% \cmidrule(lr){1-1}\cmidrule(lr){2-5}
		SAM \cite{zhao2021sam} (0.8.4) & 
		$54 \pm 3$ & $47 \pm 5$ & $29 \pm 3$ & $29 \pm 2$ \\
		LBC \cite{chen2020learning} (0.9.6) & 
		$71 \pm 5$ & $63 \pm 3$ & $51 \pm 3$ & $39 \pm 6$ \\
		LSD \cite{ohn2020learning} (0.8.4) & 
		N/A & N/A & $30 \pm 4$ & $32 \pm 3$ \\
		DA-RB\textsuperscript{+}(E) \cite{prakash2020exploring} & 
		$66 \pm 5$ & $56 \pm 1$ & $36 \pm 3$ & $35 \pm 2$ \\
		DA-RB\textsuperscript{+} \cite{prakash2020exploring} (0.8.4)  & 
		$62 \pm 1$ & $60 \pm 1$ & $34 \pm 2$ & $25 \pm 1$ \\
		Our baseline, $\mathcal{L}$ & 
		$\mathbf{88} \pm 4$ & $29 \pm 3$ & $32 \pm 11$ & $28 \pm 4$ \\
		Our improved, $\mathcal{L}_\text{D} $ & 
		$\mathbf{88} \pm 4$ & $29 \pm 3$ & $32 \pm 11$ & $28 \pm 4$ \\
		Our best, $\mathcal{L}_\text{D}+\mathcal{L}_\text{N}$ & 
		$86 \pm 5$ & $\mathbf{82} \pm 2$ & $\mathbf{78} \pm 5$ & $\mathbf{78} \pm 0$ \\
		\hline
	\end{tabular}
	\vspace{-1ex}
	\label{table:sucess_rate_nc_dense}
	\vspace{-2ex}
\end{table}

\begin{table*}
	\caption{\textbf{Driving performance and infraction analysis of BID agents on nocrash-crowded, new town \& new weather.} 
		The mean and standard deviation are reported over 5 evaluation seeds.}
	\setlength{\tabcolsep}{7.4pt}
	\centering
	\begin{tabular}{lccccccccc} 
		\hline
		& \begin{tabular}{@{}c@{}}Success \\ rate \end{tabular} 
		& \begin{tabular}{@{}c@{}}Driving \\ score \end{tabular} 
		& \begin{tabular}{@{}c@{}}Route \\ compl. \end{tabular} 
		& \begin{tabular}{@{}c@{}}Infrac. \\ penalty \end{tabular} 
		& \begin{tabular}{@{}c@{}}Collision \\ others \end{tabular} 
		& \begin{tabular}{@{}c@{}}Collision \\ pedestrian \end{tabular} 
		& \begin{tabular}{@{}c@{}}Collision \\ vehicle \end{tabular}  
		& \begin{tabular}{@{}c@{}}Red light \\ infraction \end{tabular}  
		& \begin{tabular}{@{}c@{}}Agent \\ blocked \end{tabular}  \\
		\hline
		% \cmidrule(lr){1-1}\cmidrule(lr){2-5}\cmidrule(lr){6-10}
		iter 5
		& \%, $\uparrow$
		& \%, $\uparrow$
		& \%, $\uparrow$
		& \%, $\uparrow$
		& \#/Km, $\downarrow$
		& \#/Km, $\downarrow$
		& \#/Km, $\downarrow$
		& \#/Km, $\downarrow$
		& \#/Km, $\downarrow$
		\\
		\hline
		%\cmidrule(lr){1-1}\cmidrule(lr){2-5}\cmidrule(lr){6-10}
		$\mathcal{L}_\mathrm{b}$
		& $32 \pm 5$ & $42 \pm 3$ & $61 \pm 5$ & $76 \pm 4$ 
		& $0.53 \pm 0.55$ & $\mathbf{0}\pm0$ & $0.63 \pm 0.50$ & $3.33 \pm 0.58$ & $19.4\pm 14.4$ \\
		$\mathcal{L}_\text{D}$
		& $58\pm6$ & $65\pm3$ & $85\pm2$ & $76\pm1$ 
		& $2.06\pm1.28$ & $\mathbf{0}\pm0$ & $1.37\pm1.10$ & $1.5\pm0.2$ & $2.83\pm1.46$ \\
		$\mathcal{L}_\text{N}$
		& $75\pm2$ & $78\pm0$ & $90\pm2$ & $85\pm1$ 
		& $0.51\pm0.25$ & $\mathbf{0}\pm0$ & $0.52\pm0.17$ & $0.69\pm0.06$ & $3.36\pm0.21$ \\
		$\mathcal{L}_\text{D}+\mathcal{L}_\text{N}$
		& $\mathbf{88} \pm 5$ & $\mathbf{89} \pm 3$ & $\mathbf{97} \pm 0$ & $\mathbf{90} \pm 3$ 
		& $\mathbf{0.07} \pm 0.04$ & $0.01 \pm 0.01$ & $\mathbf{0.22} \pm 0.07$ & $\mathbf{0.62} \pm 0.22$ & $\mathbf{0.83} \pm 0.03$ \\
		\hline
		%\cmidrule(lr){1-1}\cmidrule(lr){2-5}\cmidrule(lr){6-10}
		$\mathcal{L}_\text{BID}$
		& $96 \pm 2$ & $96 \pm 3$ & $99 \pm 0$ & $97 \pm 2$ 
		& $0 \pm 0$ & $0.12 \pm 0.08$ & $0.03 \pm 0.06$ & $0.14 \pm 0.18$ & $0 \pm 0$ \\
		Expert
		& $91 \pm 1$ & $78 \pm 2$ & $97 \pm 1$ & $81 \pm 2$ 
		& $0 \pm 0$ & $0 \pm 0$ & $0.19 \pm 0.07$ & $1.92 \pm 0.22$ & $0.17 \pm 0.09$\\
		\hline
	\end{tabular}
	\vspace{-1ex}
	\vspace{-2.5ex}
	\label{table:infraction}
\end{table*}


% 可视化注意力
\subsubsection{Visualizing BID's Attention}
\label{sec:Visualization}
\hspace{1pc}We are interested in understanding which parts of the image BID focuses on during decision-making. 
To achieve this, we use Grad-CAM~\cite{Selvaraju:2017}, where gradients flow from the action space to the final convolutional layer of the ResNet backbone. 
This process generates a heatmap that highlights the image regions most important for action prediction. 
However, since BID addresses a regression task, its output can take both positive and negative values, while Grad-CAM is originally designed for image classification tasks, where outputs are always positive. 
To adapt Grad-CAM for our case, we cannot simply focus on the positive gradients of the feature map. 
Instead, the computation is split into two cases based on the sign of output value. 
Negative gradients are used to compute the feature map weights when the steering angle or acceleration is negative, and positive gradients are used when the output is positive.


% _results\_results\CILv2\CILv2_3cam_single_lane\Eval\Valid_gradCAM_Roach_LBCRoutes_3cam_valid\30\-1\85.jpg
Fig.~\ref{fig:attention_ped_greed} illustrates the activation map at an intersection. 
Three regions of the image are highly activated: the lane shoulder on the right, the crossing pedestrians in the center, and the driving area on the left. 
This suggests that BID demonstrates a proper understanding of the scene and a clear connection between its observations and actions. 
Specifically, BID decides to brake because of the pedestrians, even though the traffic light is green and the navigation command is to turn left. 
This indicates that BID appropriately prioritizes the presence of pedestrians over the traffic signal and navigation command.


% 验证集的1350帧，激活图的第45张
\begin{figure}[ht!]
	\centering
	\includegraphics[width=\linewidth]{fig/attention_ped_greed.pdf}
	\caption{Activation maps of BID at an intersection in Town02 show three highly activated image areas from driving viewpoints: 
		the lane shoulder in the right, the crossing pedestrians in the center, and the driving area in the left. 
		The causality between observation and action is demonstrated by a strong braking response (0.8530) due to the presence of pedestrians, despite the ``go-straight" command. 
		This highlights BID's ability to prioritize pedestrian safety over other factors in BID's decision-making process. }
	\label{fig:attention_ped_greed}
\end{figure}



% 消融实验：速度（包括背侧通路）、导航
\subsubsection{Ablation}

\hspace{1pc}The performance of an IL agent is inherently limited by the quality of the expert it imitates. 
If the expert's performance is poor, it is not meaningful to compare IL agents trained on that expert. 
This issue becomes apparent in the nocrash new town scenario with crowded traffic, where autopilots typically perform poorly. 
To establish a high-performance upper bound and ensure a fair comparison, we training the BID (Fig.~\ref{fig:score_eu_lb_tt_tn}) and conduct ablation studies (Table~\ref{table:infraction}) under the crowded traffic setting, allowing our autopilot to achieve a driving score of 78\% and a success rate of 91\%. 
For a more meaningful comparison with state-of-the-art models, the best-performing configuration from the ablation studies is also evaluated on nocrash with crowded traffic, as shown in Table~\ref{table:sucess_rate_nc_dense}.


% 说明消融的各个指标？
% L_N 导航信号
% L_D 背侧流信号
Fig.~\ref{fig:score_eu_lb_tt_tn} shows the driving scores of experts and IL agents at each DAGGER iteration on nocrash and offline leaderBoard with crowded traffic.
The baseline $\mathcal{L}$ represents our implementation of the baseline BID trained by expert. 
According to the leaderBoard instructions, this additional navigation vector helps disambiguate situations where the semantics of left and right may be unclear due to the complexity of the map.
Given the improvements in our best-performing BID, it is expected that $\mathcal{L}_\text{D}$ and $\mathcal{L}_\text{D} + \mathcal{L}_\text{N}$ achieve higher success rates than those reported in Table~\ref{table:sucess_rate_nc_dense}.
The significant performance gap between the baseline and $\mathcal{L}_\text{D} + \mathcal{L}_\text{N}$, especially when generalizing to a new town and new weather, highlights the limitations of the baseline approach.


% 一步步添加后的效果
By incorporating the dorsal stream and speed embedding into the baseline $\mathcal{L}_\text{b}$, $\mathcal{L}_\text{D}$ outperforms $\mathcal{L}_\text{b}$ overall.
Additionally, learning from the action distribution allows $\mathcal{L}_\text{D}$ to generalize better than $\mathcal{L}_\text{b}$ on the nocrash dataset, but not on the offline leaderBoard.
Feature matching only improves performance when $E_\text{s}$ is provided with the necessary information to produce accurate driving action.
In our case, $E_\text{N}$ contains navigational information since the desired route is given throught nagivation command.
For the leaderBoard, navigational information is partially encoded in $E_N$, which includes the vector pointing to the next desired waypoint, leading to improved performance with the use of $E_\text{N}$.
To evaluate this idea, we utilize a integrated network architecture where the measurement vector $E_\text{N}$ is augmented with the navigation command encoded as a one-hot vector.
With feature matching applied to this architecture, $\mathcal{L}_\text{BID}$ achieves the best driving score among IL agents in the nocrash new town and weather generalization test, even outperforming the autopilot.



%
Adding value supervision alongside feature matching accelerates the convergence of the DAGGER process, as demonstrated by $\mathcal{L}_\text{D} + \mathcal{L}_\text{N}$.
However, when feature matching is omitted, value supervision alone does not lead to superior performance with $\mathcal{L}_\text{D}$.
This suggests a potential synergy between feature matching and value estimation. 
Intuitively, the latent feature of roach encodes the necessary information for value estimation, so mimicking this feature should assist in predicting the value. 
In turn, value estimation can help regularize feature matching, improving overall performance.


\subsubsection{Infraction and Performance Analysis}


\hspace{1pc}Table~\ref{table:infraction} presents a detailed performance and infraction analysis on the nocrash dataset with crowded traffic in the new town and weather setting. 
Notably, the extremely high ``Agent blocked" rate in our baseline $\mathcal{L}_\text{A}(\text{a})$ is primarily caused by reflections from after-rain puddles. 
This issue is significantly reduced by imitating expert, which drives more naturally, leading to a 23\% absolute improvement in the driving score for $\mathcal{L}_\text{D}$. 
This gain reflects the benefit of using a better expert, while keeping the same imitation learning approach. 
Further improvements are achieved with the addition of soft targets and latent feature supervision in $\mathcal{L}_\text{D}+\mathcal{L}_\text{N}$, resulting in another 30\% absolute increase in performance. 
By handling red lights more effectively, this BID agent achieves a driving score of 89\%, reaching expert-level performance with just camera image as input.





 