\section{Method}

\subsection{Task Formulation}

\hspace{1pc}BID is trained using imitation learning, which can be formalized as follows:
Expert drivers take an action $\mathbf{a}_{i}$ (such as a ego-vehicle maneuver by the ego vehicle) in response to an input frame $\mathcal{F}_i$, based on the expert policy $\pi^{\star}(\mathcal{F}_{i})$ (reflecting the expert's driving skills, judgement, and behavior).
The core concept of imitation learning is to train the BID agent to replicate the expert's actions by learning from these observed interactions.


Before training the agent, we first need to collect a dataset consisting of frame/action pairs $\mathcal{D} = \left\{(\mathbf{f}_{i}, \mathbf{a}_{i})\right\}_{i=1}^N$, where each pair is generated by the expert.
This dataset is then used to train a policy $\pi_{\theta}(\mathbf{f}_{i})$, which approximateds the expert's policy.
The overall objective of imitation is 
\begin{equation} \label{eq:target}
	\argmin_{\theta} \mathbb{E}_{(\mathbf{f}_{i}, \mathbf{a}_{i})\sim \mathcal{D}} \left[ \mathcal{L}(\pi_{\theta}(\mathbf{f}_{i}), \mathbf{a}_{i}) \right] \enspace .
\end{equation}

During the testing phase, we assume that only the trained policy $\pi_{\theta}(\mathbf{f}_{i})$ will be utilized, and the expert no longer be available.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{fig/net.pdf}
	\caption{The network architecture of the BID agent is primarily composed of two main components: the perception network and the decision network.
	The perception network includes both the dorsal and ventral pathways, which allow the system to understand and process its surroundings.
	The decision network consists of five modules that represent different regions of the prefrontal cortex: the medial prefrontal cortex (VPC), orbital prefrontal cortex (OPC), caudal prefrontal cortex (CPC), dorsal prefrontal cortex (DPC), and ventral prefrontal cortex (VPC). 
	Specifically, the input to the system is an RGB image $ \mathbf{F}_{t} $, which represents the current visual scene.
	The output is the action $\mathbf{a}_{i}$, which directly controls the ego-vehicle's maneuvering.
	The action $A_\text{t}$ includes the steering angle $ A_t^s $ and acceleration $ A_t^a $ of the vehicle, making BID a pure vision-based, end-to-end autonomous driving model.
	It is important to note that these action components are automatically associated with the captured images during the data collection process.
	}
	\label{fig:fig2}
\end{figure*}


\subsection{Neural Aligned BID}
\hspace{1pc} Inspired by the brain's remarkable capabilities in perception and decision-making, we have integrated the complex information processing mechanisms of neural networks into an autonomous driving system. 
This approach is designed to significantly improve both the interpretability and robustness of the system.

\subsubsection{Perception Network}
\hspace{1pc}As shown in Fig.~\ref{fig:fig2}, our brain-inspired perception network begins by capturing precise input data from the surrounding environment, which includes essential traffic elements such as roads, vehicles, pedestrians, and traffic signals. 
This data is then processed through a series of deep convolutional neural networks and recurrent networks, incorporating activation functions commonly used in deep neural networks. 
This processing mirrors the way external information is handled by the brain's visual cortex. 
Initially, the primary visual cortex (V1) processes the visual input, extracting key features. 
These features are then passed along to the dorsal visual pathway, where areas such as the MT and MST (Middle Temporal and Middle Superior Temporal Areas) specialize in encoding motion information, particularly related to spatial location and movement. 
Simultaneously, the ventral pathway receives these features, with the IT (Inferior Temporal Cortex) focusing on encoding object category information, thereby facilitating object recognition.


% 模型的输入
% the ego vehicle's forward speed $s_t\in\mathbb{R}$
At time $t$, the current state $\mathbf{F}_t$ consists of a set of images from the left, central, and right cameras $\mathbf{F}_{t}=\{\mathbf{f}_{l,t}, \mathbf{f}_{c,t}, \mathbf{f}_{r,t} \}$, along with a high-level navigation command $\mathbf{n}_t\in\mathbb{R}^{k}$, which is encoded as a one-hot vector embedding.
%
%At time $t$, each image $\mathbf{f}_{v,t}\in \mathbb{R}^{W\times H\times 3}$ from the multi-view camera setting is processed by a share-weight ResNet34 \cite{He:2016}, pre-trained on ImageNet \cite{Deng:2009}. 
% Cornet
Each visual area in ventral pathway implements a specific neural circuitry, where neurons perform simple, canonical computations, such as convolution, addition, nonlinearity, response normalization, and pooling over a receptive field. 
The structure of the circuitry is identical across all visual areas (with the exception of V1$_\text{COR}$), though the total number of neurons in each area may vary.
To manage the high computational demands, the first area, V1$_\text{COR}$, performs a 7$\times$7 convolution with stride 2, followed by 3$\times$3 max pooling with stride 2, and another 3$\times$3 convolution. 
The subsequent areas, V2$_\text{COR}$, V4$_\text{COR}$ and IT$_\text{COR}$ , each carry out two 1$\times$1 convolutions, a bottleneck-style 3 $ \times $ 3 convolution with stride 2 (which expands the number of features by a factor of four), and another 1$\times$1 convolution. 
To introduce recurrence, the outputs of each area are fed back into the same area multiple times for further processing.


%
After this sequence of processing steps, our network is capable of efficiently extracting and outputting multi-dimensional features of the current environment to the decision network. 
These features include both the appearance characteristics of objects and motion-related information.


%As discussed in \cite{Alexey:2021}, the lack of inductive biases makes transformer models require more data to achieve good performance. 
%In order to possess the inherent properties of CNNs (\ie, exploiting locality and translation equivariance), as well as leveraging the attention mechanism of transformers, we propose to adapt the hybrid model suggested in \cite{Alexey:2021} to our case. 
%At time $t$, each image $\mathbf{f}_{v,t}\in \mathbb{R}^{W\times H\times 3}$ from the multi-view camera setting is processed by a share-weight ResNet34 \cite{He:2016}, pre-trained on ImageNet \cite{Deng:2009}. 
%Then, for each view $v$, we take the resulting feature map $\mathbf{f}_{v,t}\in\mathbb{R}^{w\times h\times c}$ from the last convolutional layer of ResNet34, where $w\times h$ is the spatial size and $c$ indicates the feature dimension. 
%Each feature map is then flattened along the spatial dimensions, resulting in $P\times c$ tokens, where $P=w*h$ is the number of spatial features per image. 
%Since we will set our cameras to a resolution of $W\times H=300\times300$ pixels, for each one we obtain $P=100$ patches with $c=512$ from the ResNet34 backbone. 


\subsubsection{Decision Network}
% 决策模块
The output features from the perception network are then efficiently passed to a brain-inspired decision-making network, where they undergo further processing through transformer networks, along with anatomical alignment to the prefrontal cortex pathway. 
This simulates how sensory information is processed by different regions of the prefrontal cortex in the brain.
%


% 腹侧
The Ventral Prefrontal Cortex (VPC) generates goals based on visual cues, allowing for quick responses and adaptation to changes in the environment.
%
In our setup, with $|\mathbf{F}_{t}|=3$ views (left, central, and right cameras), we flatten the image patches from each view and tokenize them into a sequence of length $S=|\mathbf{F}_t|*w*h$, which is then fed into the transformer model.
To provide positional context for each token, we apply a standard learnable 1D positional embedding $\mathbf{p}\in \mathbb{R}^{S\times c}$, as done in \cite{Alexey:2021}, and add this embedding directly to the token.


% 背侧
The Dorsal Prefrontal Cortex (DPC) is responsible for generating goals based on sequences, as well as temporal and spatial environmental cues, offering clear guidance for decision-making and action.
% 眶额
Within this process, the Orbitofrontal Cortex (OFC) plays a crucial role in establishing future action goals, helping to clarify and focus on the desired objectives. 
% 速度、导航命令
The navigation command $\mathbf{n}_t$ and forward speed $s_t$ are projected linearly into a vector space $\mathbb{R}^{c}$ using a fully connected layer.
The resulting state embedding $\mathbf{z}_t$ is then derived by adding these input embeddings together.


% 内侧
The Medial Prefrontal Cortex (MPC) is critical for making decisions based on past behavioral outcomes, ensuring that our actions are optimized according to previous experiences.
Formally, we define the state encoder for the current state $\mathcal{X}_t=(\mathbf{F}_t, s_t, \mathbf{n}_t)$, parameterized by $\theta$, as
\begin{equation}\label{eq:encoder}
	e_{\theta}: \mathbb{R}^{|\mathbf{X}_{t}|\times W\times H\times3}\times\mathbb{R}\times \mathbb{R}^k \rightarrow \mathbb{R}^{S \times c} \enspace .
\end{equation}


In CIL \cite{Codevilla:2018} and CILRS \cite{Codevilla:2019}, $\mathbf{c}_{t}$ is used as a switch to activate different MLP branches for predicting $\mathbf{a}_{t}$. 
However, in our approach, we treat $\mathbf{c}_{t}$ as an input signal to be processed by a transformer. 
We found no significant differences between these two methods, and treating $\mathbf{c}_{t}$ as an input signal simplifies the training process.


% 注意力学习
% 连接3个图像
To effectively associate intra-view and inter-view information, we leverage the attention mechanism of transformers\cite{Vaswani:2017}. 
This approach allows the model to learn the relationships between distant image patches (tokens), enabling it to associate feature map patches across different views ({\ie}, from the left, center, and right perspectives of the ego-vehicle). 


In the embedded space $\mathbf{Z}_{t}$, we learn a scenario embedding using a transformer encoder composed of $L$ multi-head attention layers. 
Each layer includes Multi-headed Self-Attention (MHSA) \cite{Vaswani:2017}, layer normalization (LN) \cite{Ba:2016}, and feed-forward MLP blocks.  
The final output is obtained by linearly projecting the concatenated outputs of each attention head, which are then passed to the action prediction module. 
We use $L=4$ layers, each with $4$ attention heads. 
The hidden dimension $D$ of the transformer layer is set to match the output dimension of the ResNet, {\ie}, $D=512$. 


% 尾侧
Meanwhile, the CPC (Caudal Prefrontal Cortex) is crucial for searching, identifying, and locating specific targets, ensuring that we can accurately focus on and navigate towards these targets.  
% 动作预测
The output from the transformer encoder, with a size of $S\times c$, is average-pooled and passed through an MLP.
This MLP consists of three fully connected layers (FC), with ReLU nonlinearity applied between each layer.
The final output action $\mathbf{a}_t\in\mathbb{R}^2$ represents the steering angle and acceleration (brake/throttle), {\ie}, $\mathbf{a}_{t} = (a_{\text{s},t}, a_{\text{acc}, t})$. 


% 总结
These regions work together in the decision-making process, forming a highly integrated latent feature vector that effectively captures the essential information needed for making decisions. 
This latent vector is then processed through a carefully constructed hidden layer, which generates accurate and reliable outputs, such as driving actions, value function estimates, and speed control commands.


\subsection{Loss Function}

\hspace{1pc}The BID network is designed to replicate the advanced information processing abilities of the human brain by carefully aligning brain pathways with corresponding network modules. 
Through iterative updates, the network adjusts its parameters to better match human decision-making processes, ultimately improving its performance and accuracy.
%
% 损失函数
At time $t$, given a predicted action $\mathbf{a}_{t}$ and the expert's action(ground truth action) $\hat{\mathbf{a}}_{t}$, we define the training loss as:
\begin{equation}\label{eq:loss}
	\mathcal{L}(\mathbf{a}_t, \hat{\mathbf{a}}_t) = \lambda_{\text{acc}}\lVert a_{\text{acc},t}-\hat{a}_{\text{acc,t}}\rVert_{1} + \lambda_{\text{s}} \lVert a_{\text{s},t}-\hat{a}_{\text{s},t} \rVert_{1} \enspace ,
\end{equation}
where $\lVert\cdot\rVert_{1}$ denotes the $L_1$ distance, $\lambda_{acc}$ and $\lambda_s$ represent the weights assigned to the acceleration and steering angle loss, respectively.
In our setup, both steering angle and acceleration are constrained within the range of $[-1, 1]$, with negative values indicating braking and positive values corresponding to throttle.
The weights are set as $\lambda_{acc} = \lambda_{s} = 0.5$. 


In CILRS \cite{Codevilla:2019}, speed prediction regularization is incorporated into the training loss to address the inertia issue caused by the high likelihood of the ego vehicle remaining stationary in the training data.
However, we did not encounter this problem in our approach, and therefore, the speed prediction branch is not included in our model.
Our results demonstrate that a simple $L_1$ loss is sufficient to achieve strong performance, even in unfamiliar environments.
%
Additionally, the outputs from both the brain-inspired perception network and the brain-inspired decision network are concatenated to form a latent feature that captures critical driving-related information. 
This latent feature is then passed through a hidden layer to map it to driving actions, which also incorporates a feature loss.


%\subsubsection{Brain-Expert Mimetic Entity}
%
%\hspace{1pc}\textbf{\textsf{Agent.}} The BID agent utilizes imitation learning to predict driving behavior based on the current state, supervised by extensive data generated by the reinforcement learning expert. The structure of the agent is illustrated in Fig. \ref{fig:fig2}.

%\textbf{\textsf{Loss Function.}} 
%In the decision-making stage, for each command, a branch is constructed. 
%All branches share the same architecture, with each branch containing an action head for predicting the continuous action $\mathbf{a}$ and a velocity head for predicting the current vehicle speed $\mathbf{s}$. 
%And $\mathbf{\hat{a}}$ represents the expert's action,$\mathbf{\hat{s}}$ is the measured speed, and $\mathbf{a}$ and $\mathbf{s}$ are the actions and speeds predicted by the agent, respectively. 
%\begin{align}
%   \mathcal{L} = \|\hat{\mathbf{a}}-\mathbf{a}\|_{1}^{2} + \lambda_{\mathrm{S}} \cdot (\hat{s}-s)^{2} + \lambda_{\mathrm{F}} \cdot \left\| \mathbf{j}_{\mathrm{NR}}-\mathbf{j}_{\mathrm{BID}} \right\|_{2}^{2}
%\end{align}


\begin{comment}
\subsection{Similarity Metrics}
\hspace{1pc}\textbf{\textsf{Collecting Brain Activation Signals.}} To align the network, we collect the brain's responses to multi-channel grayscale images. Initially, EEG sensors are placed at appropriate locations and connected to data acquisition equipment. Subsequently, neuroimaging software is utilized to preprocess cortical activation data. Then, the brain data is aligned with standard data to ensure consistency, followed by noise removal and data smoothing. Ultimately, we obtain activation signals from visual brain regions that vary over time as participants view different images. These signals are further amplified through deconvolution processing to reveal the brain's activation patterns.

\textbf{\textsf{Network Alignment.}} To achieve a closer alignment with human brain functions, the network parameters are continuously optimized, thereby significantly enhancing the bionic performance and simulation accuracy of the network.


% 性能度量标准
\textbf{\textsf{Metrics:}}
Our results are reported in success rate, the metric proposed by NoCrash, and driving score, a new metric introduced by the CARLA LeaderBoard. 
The success rate is the percentage of routes completed without collision or blockage. 
The driving score is defined as the product of route completion, the percentage of route distance completed, and infraction penalty, a discount factor that aggregates all triggered infractions.
For example, if the agent ran two red lights in one route and the penalty coefficient for running one red light was $0.7$, then the infraction penalty would be  $0.7^{2}$$=$$0.49$.
Compared to the success rate, the driving score is a fine-grained metric that considers more kinds of infractions and it is better suited to evaluate long-distance routes.
More details about the benchmarks and the complete results are found in the supplement.


\textbf{\textsf{Similarity Assessment.}} After parameter tuning, the BID network is capable of simulating the mechanisms of the human brain in environmental recognition and decision-making. Following each optimization, we accurately calculate the similarity between the BID and the human brain. The degree of congruency between the brain-inspired network and the human brain is quantified by computing the average of activation similarity and decision similarity through the specified formula. Notably, $S_{X_{p}}$ and $S_{X_{d}}$ denote the standard deviations of the model's activation sample points and decision sample points, respectively, whereas $S_{Y}$ represents the corresponding data values of the human brain.
\begin{align}
	r & = \frac{1}{2}\left(\frac{Cov_{X_{p},Y_{p}}}{\sqrt{S_{X_{p}} S_{Y_{p}}}}+\frac{Cov_{X_{d},Y_{d}}}{\sqrt{S_{X_{d}} S_{Y_{d}}}}\right)
\end{align}
%\begin{align}
%	r & = \frac{1}{2}\left(\frac{\sum_{i = 1}^{n}\left(X_{p_{i}}-\bar{X_{p}}\right)\left(Y_{p_{i}}-\bar{Y_{p}}\right)}{\sqrt{\sum_{i = 1}^{n}\left(X_{p_{i}}-\bar{X_{p}}\right)^{2}} \sqrt{\sum_{i = 1}^{n}\left(Y_{p_{i}}-\bar{Y_{p}}\right)^{2}}} \right. \nonumber \\
%	& \qquad \left. + \frac{\sum_{i = 1}^{n}\left(X_{d_{i}}-\bar{X_{d}}\right)\left(Y_{d_{i}}-\bar{Y_{d}}\right)}{\sqrt{\sum_{i = 1}^{n}\left(X_{d_{i}}-\bar{X_{d}}\right)^{2}} \sqrt{\sum_{i = 1}^{n}\left(Y_{d_{i}}-\bar{Y_{d}}\right)^{2}}} \right)
%\end{align}

\end{comment}