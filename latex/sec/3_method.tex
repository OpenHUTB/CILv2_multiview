\section{Method}

\subsection{Task Formulation}

\hspace{1pc}BID is trained by imitation learning, which we formalized as follows.
Expert drivers produce an action $\mathbf{a}_{i}$ (ego-vehicle maneuver) when encountering an input frame $\mathcal{F}_i$, (image frame, signals) given the expert policy $\pi^{\star}(\mathcal{F}_{i})$ (driving skills, attitude, etc.).
The basic idea behind imitation is to train an agent (here BID) that mimics an expert by using these observation.


Prior to training an agent, we need to collect a dataset comprised of frame/action pairs $\mathcal{D} = \left\{(\mathbf{f}_{i}, \mathbf{a}_{i})\right\}_{i=1}^N$ generated by the expert.
This dataset is in turn used to train a policy $\pi_{\theta}(\mathbf{f}_{i})$ which approximateds the expert policy.
The general imitation learning objective is the 
\begin{equation} \label{eq:target}
	\argmin_{\theta} \mathbb{E}_{(\mathbf{f}_{i}, \mathbf{a}_{i})\sim \mathcal{D}} \left[ \mathcal{L}(\pi_{\theta}(\mathbf{f}_{i}), \mathbf{a}_{i}) \right] \enspace .
\end{equation}

During testing time, we assume that only the trained policy $\pi_{\theta}(\mathbf{f}_{i})$ will be used and no expert will be available.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{fig/net.pdf}
	\caption{Network architecture of the BID agent:
	our model is mainly comprised of two parts: perception and decision network.
	Perception network include dorsal and ventral pathway to understand their surroundings.
	Decision network include five modules: medial prefrontal cortex (VPC), orbital prefrontal cortex (OPC), caudal prefrontal crotex (CPC), dorsal prefrontal cortex (DPC), ventral prefrontal cortex (VPC).
	Specifically, the input observation consists of RGB image $ \mathbf{F}_{t} $.
	The output is action $\mathbf{a}_{i}$, which is directly applied to maneuver to ego-vehicle.
	Since $a_t$ consists of the ego-vehicle's steering angle and acceleration, BID is a vision-based vision-based pure end-to-end AD model.
	Note that these action components are automatically read and associated with the captured images during data collection.
	}
	\label{fig:fig2}
\end{figure*}


\subsection{Neural Aligned BID}
\hspace{1pc}Drawing inspiration from the brain's exceptional mechanisms in perception and decision-making, we have seamlessly integrated the intricate methods of information processing by brain neurons into an autonomous driving system, aiming to significantly enhance the system's interpretability and robustness.

\subsubsection{Perception Network}
\hspace{1pc}As depicted in Fig.~\ref{fig:fig2}, our brain-inspired perception network initially captures precise input data from the surrounding environment, encompassing crucial traffic elements such as roads, vehicles, pedestrians, and traffic signals. 
Subsequently, this data undergoes meticulous processing through a series of deep convolutional neural networks and recurrent networks, along with the utilization of deep neural network activation functions. 
This processing mimics the handling of external information by various pathways in the brain's visual cortex. 
Initially, the primary visual cortex (V1) performs initial processing of visual information, extracting salient features.
Two adjacent frame embedding are then relayed to the dorsal visual pathway, where the MT/MST (Middle Temporal and Middle Superior Temporal Areas) encode motion features, specializing in the processing of information related to spatial location and movement.
Concurrently, the ventral pathway receives these features, and the IT (Inferior Temporal Cortex) encodes target category information, facilitating object recognition. 


% 模型的输入
% the ego vehicle's forward speed $s_t\in\mathbb{R}$
At time $t$, the current state $\mathbf{F}_t$ consists of a set of images from the left, central, and right cameras $\mathbf{F}_{t}=\{\mathbf{f}_{l,t}, \mathbf{f}_{c,t}, \mathbf{f}_{r,t} \}$ and a high-level navigation command $\mathbf{n}_t\in\mathbb{R}^{k}$ which is encoded as a one-hot vector embedding.
%
%At time $t$, each image $\mathbf{f}_{v,t}\in \mathbb{R}^{W\times H\times 3}$ from the multi-view camera setting is processed by a share-weight ResNet34 \cite{He:2016}, pre-trained on ImageNet \cite{Deng:2009}. 
% Cornet
Each visual area in ventral pathway implements a particular neural circuitry with neurons performing simple canonical computations: convolution, addition, nonlinearity, response normalization or pooling over a receptive field. 
The circuitry is identical in each of its visual areas (except for V1$_\text{COR}$), but we vary the total number of neurons in each area.
Due to high computational demands, first area V1$_\text{COR}$ performs a 7 $ \times $ 7 convolution with stride 2, 3 $ \times $ 3 max pooling with stride 2, and a 3 $ \times $ 3 convolution. 
Areas V2$_\text{COR}$, V4$_\text{COR}$ and IT$_\text{COR}$ perform two 1 $ \times $ 1 convolutions, a bottleneck-style 3 $ \times $ 3 convolution with stride 2, expanding the number of features fourfold, and a 1 $ \times $ 1 convolution. 
To implement recurrence, outputs of an area are passed through that area several times. 


%
After this series of processing, our network is able to effectively extract and output multi-dimensional features of the current environment to decision network, including the appearance features of objects, motion features.


%As discussed in \cite{Alexey:2021}, the lack of inductive biases makes transformer models require more data to achieve good performance. 
%In order to possess the inherent properties of CNNs (\ie, exploiting locality and translation equivariance), as well as leveraging the attention mechanism of transformers, we propose to adapt the hybrid model suggested in \cite{Alexey:2021} to our case. 
%At time $t$, each image $\mathbf{f}_{v,t}\in \mathbb{R}^{W\times H\times 3}$ from the multi-view camera setting is processed by a share-weight ResNet34 \cite{He:2016}, pre-trained on ImageNet \cite{Deng:2009}. 
%Then, for each view $v$, we take the resulting feature map $\mathbf{f}_{v,t}\in\mathbb{R}^{w\times h\times c}$ from the last convolutional layer of ResNet34, where $w\times h$ is the spatial size and $c$ indicates the feature dimension. 
%Each feature map is then flattened along the spatial dimensions, resulting in $P\times c$ tokens, where $P=w*h$ is the number of spatial features per image. 
%Since we will set our cameras to a resolution of $W\times H=300\times300$ pixels, for each one we obtain $P=100$ patches with $c=512$ from the ResNet34 backbone. 


\subsubsection{Decision Network}
% 决策模块
These features are then efficiently transmitted to a brain-inspired decision-making network, undergoing further processing through convolutional and recurrent networks, as well as optimization via deep neural network activation functions. 
This simulates the processing of sensory outcomes by various regions of the prefrontal cortex in the brain. 
%


% 腹侧
The VPC (Ventral Prefrontal Cortex) generates goals based on visual cues, enabling us to respond quickly and adapt to changes in the external environment.  
%
Since we use $|\mathbf{X}_{t}|=3$ views (left, central, and right cameras), we take the flattened patches for each view, and tokenize them as the whole sequence with length $S=|\mathbf{X}_t|*w*h$ for further feeding into the transformer model. 
To provide the positional information for each token, we apply the standard learnable 1D positional embedding $\mathbf{p}\in \mathbb{R}^{S\times c}$ as done in \cite{Alexey:2021}, which is added directly to the token. 


% 眶额
Among them, the OFC (Orbitofrontal Cortex) plays a vital role in setting future action goals, assisting us in clarifying and locking onto the desired objectives. 
% 背侧
The DPC (Dorsal Prefrontal Cortex) excels at generating goals based on sequences, temporal and spatial environments, providing clear directions and guidance for our actions.
% 速度、导航命令
The command $\mathbf{c}_t$ and forward speed $s_t$ are linearly projected to $\mathbb{R}^{c}$ using a fully connected layer.
The resulting state embedding $\mathbf{z}_t$ is obtained by the addition of these input embeddings. 


% 内侧
The MPC (Medial Prefrontal Cortex) plays a crucial role in making choices based on prior behavioral outcomes, ensuring that our behavior is optimized based on experience.
Formally, we define the state encoder of the current state $\mathcal{X}_t=(\mathbf{X}_t, s_t, \mathbf{c}_t)$, parameterized by $\theta$, as
\begin{equation}\label{eq:encoder}
	e_{\theta}: \mathbb{R}^{|\mathbf{X}_{t}|\times W\times H\times3}\times\mathbb{R}\times \mathbb{R}^k \rightarrow \mathbb{R}^{S \times c} \enspace .
\end{equation}


In CIL \cite{Codevilla:2018} and CILRS \cite{Codevilla:2019}, $\mathbf{c}_{t}$ is treated as a switcher (condition) to trigger different MLP branches for predicting $\mathbf{a}_{t}$. 
Here we treat $\mathbf{c}_{t}$ as an input signal to be later processed by a transformer. 
This is because we have not observed obvious differences between these two approaches while treating $\mathbf{c}_{t}$ as input signal simplifies the training. 


% 注意力学习
% 连接3个图像
To naturally associate intra-view and inter-view information, we adopt the attention mechanism of transformers\cite{Vaswani:2017}. 
We expect it to be effective to learn the mutual relevance between distant image patches (tokens), helping our model to associate feature map patches across views ({\ie}, coming from visual information to the left, center, and right of the ego-vehicle). 


Over the embedded space $\mathbf{Z}_{t}$, we learn a scenario embedding using a transformer encoder that consists of $L$ multi-head attention layers. 
Each one includes Multi-headed Self-Attention (MHSA) \cite{Vaswani:2017}, layer normalization (LN) \cite{Ba:2016}, and feed-forward MLP blocks.  
The final output is a linear projection of the concatenated output of each attention head, which is then fed into the action prediction module. We use $L=4$ layers, with $4$ heads each. 
The hidden dimension $D$ of the transformer layer is set to be equal to the ResNet output dimension, {\ie}, $D=512$. 


% 尾侧
Meanwhile, the CPC (Caudal Prefrontal Cortex) plays an indispensable role in searching, identifying, or locating specific targets, ensuring that we can accurately locate and focus on the targets. 
% 动作预测
The output of the transformer encoder with a size of $S\times c$ is average-pooled and fed into an MLP. 
The MLP consists of three fully connected layers (FC) with ReLu non-linearity applied between each FC. 
The final output action $\mathbf{a}_t\in\mathbb{R}^2$ comprises of the steering angle and acceleration (brake/throttle), {\ie}, $\mathbf{a}_{t} = (a_{\text{s},t}, a_{\text{acc}, t})$. 


% 总结
These regions collaborate and participate in decision-making, generating a highly integrated latent feature vector that precisely encapsulates the critical information required for decision-making. 
Finally, this latent feature vector is mapped through a carefully designed hidden layer, outputting precise and reliable decision-making information such as driving actions, value function estimates, and speed control.


\subsection{Loss Function}
The BID network aims to mimic the superior information processing capabilities of the human brain by meticulously comparing brain activation patterns with network activation patterns. 
It iteratively updates its network parameters to align network decisions more closely with the actual decision-making mechanisms of the human brain, thereby enhancing the performance and accuracy of the network. 
The specific methodology for updating the network is outlined as follows:
\begin{align}
	\theta_{k+1} & = \arg \max _{\theta} \underset{\tau \sim \pi_{\theta_{k}}}{\mathrm{E}}\left[\mathcal{L}_{\mathrm{ppo}}+\mathcal{L}_{\text {pre }}+\lambda_{\text {e }} \cdot \mathcal{L}_{\text {exp }}\right]
\end{align}


%Simulating the approach the brain acquires rewards, aiming to maximize reward-seeking behavior. 
%We train the decision network using Proximal Policy Optimization (PPO) \cite{schulman2017proximal} with clipping. 
%$\mathcal{L}_{\text {ppo }}$ serves as the dopamine reward signal. 
%It is understood that when the brain predicts an upcoming reward, the dopamine system becomes active, generating a reward signal. 
%This signal can be seen as feedback from the brain regarding specific behaviors or situations, informing the brain whether the behavior or situation is positive or worth pursuing. $\mathcal{L}_{\text {pre }}$ represents the prediction error of brain reward. This error arises when there is a deviation between the actual reward and the expected reward. In this context, the reward prediction error functions as a penalty term. Specifically, if the actual reward ($a_{r}$) falls below the expected reward ($p_{r}$), the reward prediction error is negative.Additionally, $\mathcal{L}_{\text {exp }}$ encapsulates the exploration of new actions or strategies during the learning process to obtain more rewards, with $\lambda$ serving as their weight parameter.

%Considering both the certainty of the policy (via the entropy regularization term) and the accuracy of reward prediction (via the reward prediction error term), this guides the model to learn more accurate reward predictions, thereby enhancing its performance in reinforcement learning tasks. Specifically, The constant (C) serves to adjust the numerical range or optimization scale of the entropy.
%\begin{align}
%	\mathcal{L}_{\text{pre}} & = -\lambda_{\text {p }} \cdot [\mathrm{H}\left(\pi_{\theta}\left(\cdot \mid \mathbf{i}_{\mathrm{NR}}, \mathbf{m}_{\mathrm{NR}}\right)\right) - (a_{r} - p_{r})^{2}]
%\end{align}
%\begin{align}
%	\mathrm{H}\left(\pi_{\theta}\right) & = -\mathrm{KL}\left(\pi_{\theta} \| \mathcal{U}(-1,1)\right)+C
%\end{align}
%
%Regularizing the whole trajectory to better utilize previous experience and improve the efficiency and stability of learning.
%\begin{align}
%    \mathcal{L}_{\text {exp }}=\sum_{k=1}^{T} f(k) \cdot \mathrm{KL}\left(\pi_{\theta}\left(\cdot \mid \mathbf{i}_{\mathrm{NR}, k}, \mathbf{m}_{\mathrm{NR}, k}\right) \| p_{z}\right)
%\end{align}


% 损失函数
At time $t$, given a predicted action $\mathbf{a}_{t}$ and a ground truth action $\hat{\mathbf{a}}_{t}$, we define the training loss as:
\begin{equation}\label{eq:loss}
	\mathcal{L}(\mathbf{a}_t, \hat{\mathbf{a}}_t) = \lambda_{\text{acc}}\lVert a_{\text{acc},t}-\hat{a}_{\text{acc,t}}\rVert_{1} + \lambda_{\text{s}} \lVert a_{\text{s},t}-\hat{a}_{\text{s},t} \rVert_{1} \enspace ,
\end{equation}
where $\lVert\cdot\rVert_{1}$ is the $L_1$ distance, $\lambda_{acc}$ and $\lambda_s$ indicate the weights given to the acceleration and steering angle loss, respectively. 
In our case, we consider steering angle and acceleration to be both in the range of $[-1, 1]$. 
Negative values of the acceleration correspond to braking, while positive ones to throttle.
The weights are set to $\lambda_{acc} = \lambda_{s} = 0.5$. 


In CILRS \cite{Codevilla:2019}, speed prediction regularization is applied in the training loss to avoid the inertia problem caused by the overwhelming probability of ego staying static in the training data. 
We do not observe this problem in our case, thus the speed prediction branch is not applied in our setting. 
Our result suggests that a simple $L_1$ loss is able to provide compelling performance, even in a new town.


%\subsubsection{Brain-Expert Mimetic Entity}
%
%\hspace{1pc}\textbf{\textsf{Agent.}} The BID agent utilizes imitation learning to predict driving behavior based on the current state, supervised by extensive data generated by the reinforcement learning expert. The structure of the agent is illustrated in Fig. \ref{fig:fig2}.

%\textbf{\textsf{Loss Function.}} 
In the decision-making stage, for each command, a branch is constructed. 
All branches share the same architecture, with each branch containing an action head for predicting the continuous action $\mathbf{a}$ and a velocity head for predicting the current vehicle speed $\mathbf{s}$. 
And $\mathbf{\hat{a}}$ represents the expert's action,$\mathbf{\hat{s}}$ is the measured speed, and $\mathbf{a}$ and $\mathbf{s}$ are the actions and speeds predicted by the agent, respectively. 
\begin{align}
   \mathcal{L} = \|\hat{\mathbf{a}}-\mathbf{a}\|_{1}^{2} + \lambda_{\mathrm{S}} \cdot (\hat{s}-s)^{2} + \lambda_{\mathrm{F}} \cdot \left\| \mathbf{j}_{\mathrm{NR}}-\mathbf{j}_{\mathrm{BID}} \right\|_{2}^{2}
\end{align}

Furthermore, the outputs of the brain-inspired perception network and the brain-inspired decision network are concatenated to produce a latent feature that encapsulates essential information for driving. 
This latent feature is then processed through a hidden layer to map it to driving actions. 
Hence, it also includes a feature loss.

\begin{comment}
\subsection{Similarity Metrics}
\hspace{1pc}\textbf{\textsf{Collecting Brain Activation Signals.}} To align the network, we collect the brain's responses to multi-channel grayscale images. Initially, EEG sensors are placed at appropriate locations and connected to data acquisition equipment. Subsequently, neuroimaging software is utilized to preprocess cortical activation data. Then, the brain data is aligned with standard data to ensure consistency, followed by noise removal and data smoothing. Ultimately, we obtain activation signals from visual brain regions that vary over time as participants view different images. These signals are further amplified through deconvolution processing to reveal the brain's activation patterns.

\textbf{\textsf{Network Alignment.}} To achieve a closer alignment with human brain functions, the network parameters are continuously optimized, thereby significantly enhancing the bionic performance and simulation accuracy of the network.


% 性能度量标准
\textbf{\textsf{Metrics:}}
Our results are reported in success rate, the metric proposed by NoCrash, and driving score, a new metric introduced by the CARLA LeaderBoard. 
The success rate is the percentage of routes completed without collision or blockage. 
The driving score is defined as the product of route completion, the percentage of route distance completed, and infraction penalty, a discount factor that aggregates all triggered infractions.
For example, if the agent ran two red lights in one route and the penalty coefficient for running one red light was $0.7$, then the infraction penalty would be  $0.7^{2}$$=$$0.49$.
Compared to the success rate, the driving score is a fine-grained metric that considers more kinds of infractions and it is better suited to evaluate long-distance routes.
More details about the benchmarks and the complete results are found in the supplement.


\textbf{\textsf{Similarity Assessment.}} After parameter tuning, the BID network is capable of simulating the mechanisms of the human brain in environmental recognition and decision-making. Following each optimization, we accurately calculate the similarity between the BID and the human brain. The degree of congruency between the brain-inspired network and the human brain is quantified by computing the average of activation similarity and decision similarity through the specified formula. Notably, $S_{X_{p}}$ and $S_{X_{d}}$ denote the standard deviations of the model's activation sample points and decision sample points, respectively, whereas $S_{Y}$ represents the corresponding data values of the human brain.
\begin{align}
	r & = \frac{1}{2}\left(\frac{Cov_{X_{p},Y_{p}}}{\sqrt{S_{X_{p}} S_{Y_{p}}}}+\frac{Cov_{X_{d},Y_{d}}}{\sqrt{S_{X_{d}} S_{Y_{d}}}}\right)
\end{align}
%\begin{align}
%	r & = \frac{1}{2}\left(\frac{\sum_{i = 1}^{n}\left(X_{p_{i}}-\bar{X_{p}}\right)\left(Y_{p_{i}}-\bar{Y_{p}}\right)}{\sqrt{\sum_{i = 1}^{n}\left(X_{p_{i}}-\bar{X_{p}}\right)^{2}} \sqrt{\sum_{i = 1}^{n}\left(Y_{p_{i}}-\bar{Y_{p}}\right)^{2}}} \right. \nonumber \\
%	& \qquad \left. + \frac{\sum_{i = 1}^{n}\left(X_{d_{i}}-\bar{X_{d}}\right)\left(Y_{d_{i}}-\bar{Y_{d}}\right)}{\sqrt{\sum_{i = 1}^{n}\left(X_{d_{i}}-\bar{X_{d}}\right)^{2}} \sqrt{\sum_{i = 1}^{n}\left(Y_{d_{i}}-\bar{Y_{d}}\right)^{2}}} \right)
%\end{align}

\end{comment}